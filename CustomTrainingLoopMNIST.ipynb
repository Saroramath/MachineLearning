{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CustomTrainingLoopMNIST.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Saroramath/MachineLearning/blob/main/CustomTrainingLoopMNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUR1q6tCEImm"
      },
      "source": [
        "This notebook illustrates the use of custom training loops for Tensorflow/Keras. While many models can be trained successfully using the high-level `.fit()` method, more advanced architectures such as *generative adversarial networks* (Part 6 of this course) and *deep reinforcement learning* (Part 9 of this course) require custom training loops. We illustrate this here for our simple MNIST model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Hct7P174L4N"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "import tensorflow_datasets as tfds\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAB6JWQ_4U3K"
      },
      "source": [
        "# Load the raw data\r\n",
        "(ds_train, ds_val), ds_info = tfds.load(\r\n",
        "    'mnist',\r\n",
        "    split=['train', 'test'],\r\n",
        "    shuffle_files=True,\r\n",
        "    as_supervised=True,\r\n",
        "    with_info=True,\r\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tu35v2wY4XoQ"
      },
      "source": [
        "# Build the training/testing pipelines\r\n",
        "def normalize_img(image, label):\r\n",
        "  return tf.cast(image, tf.float32) / 255., label #tf.one_hot(label, 10)\r\n",
        "\r\n",
        "ds_train = ds_train.map(\r\n",
        "    normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\r\n",
        "ds_train = ds_train.cache()\r\n",
        "ds_train = ds_train.shuffle(ds_info.splits['train'].num_examples)\r\n",
        "ds_train = ds_train.batch(128)\r\n",
        "ds_train = ds_train.prefetch(tf.data.experimental.AUTOTUNE)\r\n",
        "\r\n",
        "ds_val = ds_val.map(\r\n",
        "    normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\r\n",
        "ds_val = ds_val.cache()\r\n",
        "ds_val = ds_val.batch(128)\r\n",
        "ds_val = ds_val.prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kpt8VhU-4a3q",
        "outputId": "6b29836e-0aa6-4973-b472-accf5bdf3a0b"
      },
      "source": [
        "# Create the model\r\n",
        "inp = tf.keras.layers.Input(shape=(28,28,))\r\n",
        "b = tf.keras.layers.Flatten()(inp)\r\n",
        "b = tf.keras.layers.Dense(128, activation='relu')(b)\r\n",
        "out = tf.keras.layers.Dense(10, activation='softmax')(b)\r\n",
        "\r\n",
        "model = tf.keras.models.Model(inp, out)\r\n",
        "\r\n",
        "# Summary of the model\r\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 28, 28)]          0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               100480    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 101,770\n",
            "Trainable params: 101,770\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVM5tkCBE8Uj"
      },
      "source": [
        "# Define the loss function and optimizer (but we do not use the model.compile method)\r\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy()\r\n",
        "optimizer = tf.keras.optimizers.SGD()\r\n",
        "\r\n",
        "# Use accuracy as metrics (both for training and validation)\r\n",
        "train_metric = tf.keras.metrics.SparseCategoricalAccuracy()\r\n",
        "validation_metric = tf.keras.metrics.SparseCategoricalAccuracy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7oVGsQ_704-D",
        "outputId": "b18209e3-ae12-43a9-ade3-352f8c9a9a4f"
      },
      "source": [
        "# This is a custom training loop\r\n",
        "epochs = 10\r\n",
        "\r\n",
        "for epoch in range(epochs):\r\n",
        "  print(\"Working on epoch {} out of {} epochs:\\n\".format(epoch, epochs))\r\n",
        "  start_time = time.time()\r\n",
        "\r\n",
        "  # Now iterate over all batches\r\n",
        "  for batch_nr, (x_batch, y_batch) in enumerate(ds_train):\r\n",
        "\r\n",
        "    # Record operations of the forward pass in a GradientTape\r\n",
        "    with tf.GradientTape() as tape:\r\n",
        "\r\n",
        "      # Evaluate the model on the current mini-batch\r\n",
        "      y_batch_predict = model(x_batch, training=True)\r\n",
        "\r\n",
        "      # Loss over the current mini-batch\r\n",
        "      batch_loss = loss(y_batch, y_batch_predict)\r\n",
        "\r\n",
        "    # Compute the gradient of the loss function (using backpropagation)\r\n",
        "    gradients = tape.gradient(batch_loss, model.trainable_weights)\r\n",
        "\r\n",
        "    # Take one step of gradient descent\r\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_weights))\r\n",
        "\r\n",
        "    # Update the metric\r\n",
        "    train_metric.update_state(y_batch, y_batch_predict)\r\n",
        "\r\n",
        "    # Record loss value every 100 batches\r\n",
        "    if batch_nr % 100 == 0:\r\n",
        "      print(\"Current training loss for batch {} is {: 2.4f}.\".format(batch_nr,\r\n",
        "                                                              batch_loss))\r\n",
        "          \r\n",
        "  # Accuracy at the end of the epoch\r\n",
        "  current_acc = train_metric.result()\r\n",
        "  print(\"\\nTraining accuracy: {: .4f}\".format(current_acc))\r\n",
        "\r\n",
        "  # Reset the metric at epoch's end\r\n",
        "  train_metric.reset_states()\r\n",
        "\r\n",
        "  # Validation accuracy at the end of the epoch\r\n",
        "  for x_batch_val, y_batch_val in ds_val:\r\n",
        "    y_batch_val_predict = model(x_batch_val, training=False)\r\n",
        "    validation_metric.update_state(y_batch_val, y_batch_val_predict)\r\n",
        "  val_acc = validation_metric.result()\r\n",
        "  validation_metric.reset_states()\r\n",
        "  print(\"Validation accuracy: {: .4f}\".format(val_acc))\r\n",
        "  print(\"Time to complete: {: 2.4f}\\n\".format(time.time()-start_time))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Working on epoch 0 out of 10 epochs:\n",
            "\n",
            "Current training loss for batch 0 is  2.4852.\n",
            "Current training loss for batch 100 is  1.5337.\n",
            "Current training loss for batch 200 is  1.0508.\n",
            "Current training loss for batch 300 is  0.8573.\n",
            "Current training loss for batch 400 is  0.7124.\n",
            "\n",
            "Training accuracy:  0.7278\n",
            "Validation accuracy:  0.8584\n",
            "Time to complete:  8.4231\n",
            "\n",
            "Working on epoch 1 out of 10 epochs:\n",
            "\n",
            "Current training loss for batch 0 is  0.6740.\n",
            "Current training loss for batch 100 is  0.5568.\n",
            "Current training loss for batch 200 is  0.5856.\n",
            "Current training loss for batch 300 is  0.4551.\n",
            "Current training loss for batch 400 is  0.4806.\n",
            "\n",
            "Training accuracy:  0.8662\n",
            "Validation accuracy:  0.8882\n",
            "Time to complete:  2.9368\n",
            "\n",
            "Working on epoch 2 out of 10 epochs:\n",
            "\n",
            "Current training loss for batch 0 is  0.5966.\n",
            "Current training loss for batch 100 is  0.5237.\n",
            "Current training loss for batch 200 is  0.4966.\n",
            "Current training loss for batch 300 is  0.4819.\n",
            "Current training loss for batch 400 is  0.4431.\n",
            "\n",
            "Training accuracy:  0.8854\n",
            "Validation accuracy:  0.8982\n",
            "Time to complete:  2.9600\n",
            "\n",
            "Working on epoch 3 out of 10 epochs:\n",
            "\n",
            "Current training loss for batch 0 is  0.3243.\n",
            "Current training loss for batch 100 is  0.4042.\n",
            "Current training loss for batch 200 is  0.4436.\n",
            "Current training loss for batch 300 is  0.4775.\n",
            "Current training loss for batch 400 is  0.2617.\n",
            "\n",
            "Training accuracy:  0.8943\n",
            "Validation accuracy:  0.9067\n",
            "Time to complete:  2.9747\n",
            "\n",
            "Working on epoch 4 out of 10 epochs:\n",
            "\n",
            "Current training loss for batch 0 is  0.2742.\n",
            "Current training loss for batch 100 is  0.5229.\n",
            "Current training loss for batch 200 is  0.3786.\n",
            "Current training loss for batch 300 is  0.3171.\n",
            "Current training loss for batch 400 is  0.3032.\n",
            "\n",
            "Training accuracy:  0.8996\n",
            "Validation accuracy:  0.9113\n",
            "Time to complete:  2.9285\n",
            "\n",
            "Working on epoch 5 out of 10 epochs:\n",
            "\n",
            "Current training loss for batch 0 is  0.4205.\n",
            "Current training loss for batch 100 is  0.2328.\n",
            "Current training loss for batch 200 is  0.4130.\n",
            "Current training loss for batch 300 is  0.3780.\n",
            "Current training loss for batch 400 is  0.3036.\n",
            "\n",
            "Training accuracy:  0.9044\n",
            "Validation accuracy:  0.9153\n",
            "Time to complete:  2.9111\n",
            "\n",
            "Working on epoch 6 out of 10 epochs:\n",
            "\n",
            "Current training loss for batch 0 is  0.2935.\n",
            "Current training loss for batch 100 is  0.4665.\n",
            "Current training loss for batch 200 is  0.2959.\n",
            "Current training loss for batch 300 is  0.5069.\n",
            "Current training loss for batch 400 is  0.3105.\n",
            "\n",
            "Training accuracy:  0.9086\n",
            "Validation accuracy:  0.9161\n",
            "Time to complete:  2.9331\n",
            "\n",
            "Working on epoch 7 out of 10 epochs:\n",
            "\n",
            "Current training loss for batch 0 is  0.3971.\n",
            "Current training loss for batch 100 is  0.3739.\n",
            "Current training loss for batch 200 is  0.3424.\n",
            "Current training loss for batch 300 is  0.3187.\n",
            "Current training loss for batch 400 is  0.2625.\n",
            "\n",
            "Training accuracy:  0.9123\n",
            "Validation accuracy:  0.9196\n",
            "Time to complete:  2.9208\n",
            "\n",
            "Working on epoch 8 out of 10 epochs:\n",
            "\n",
            "Current training loss for batch 0 is  0.2979.\n",
            "Current training loss for batch 100 is  0.2527.\n",
            "Current training loss for batch 200 is  0.3574.\n",
            "Current training loss for batch 300 is  0.3007.\n",
            "Current training loss for batch 400 is  0.2680.\n",
            "\n",
            "Training accuracy:  0.9153\n",
            "Validation accuracy:  0.9217\n",
            "Time to complete:  2.9385\n",
            "\n",
            "Working on epoch 9 out of 10 epochs:\n",
            "\n",
            "Current training loss for batch 0 is  0.2586.\n",
            "Current training loss for batch 100 is  0.2051.\n",
            "Current training loss for batch 200 is  0.2212.\n",
            "Current training loss for batch 300 is  0.3619.\n",
            "Current training loss for batch 400 is  0.3603.\n",
            "\n",
            "Training accuracy:  0.9181\n",
            "Validation accuracy:  0.9243\n",
            "Time to complete:  2.8988\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohzKySCxC_Jm"
      },
      "source": [
        "The above code uses Tensorflow 2.0's default *eager execution*, which does not build a graph of the model, but rather evaluates each expression immediately. While this is great for debugging, it incurs significant overheads as the execution is agnostic to any potential boilerplates that could be reduced were the model to be compiled as a graph.\r\n",
        "\r\n",
        "We can compile the model into a static graph by adding the `@tf.function` decorator on it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jm6wFEgXBXmp"
      },
      "source": [
        "# Define each mini-batch gradient descent step as separate function\r\n",
        "@tf.function\r\n",
        "def train_step(x_batch, y_batch):\r\n",
        "\r\n",
        "  # Record operations of the forward pass in a GradientTape\r\n",
        "  with tf.GradientTape() as tape:\r\n",
        "\r\n",
        "    # Evaluate the model on the current mini-batch\r\n",
        "    y_batch_predict = model(x_batch, training=True)\r\n",
        "\r\n",
        "    # Loss over the current mini-batch\r\n",
        "    batch_loss = loss(y_batch, y_batch_predict)\r\n",
        "\r\n",
        "  # Compute the gradient of the loss function (using backpropagation)\r\n",
        "  gradients = tape.gradient(batch_loss, model.trainable_weights)\r\n",
        "\r\n",
        "  # Take one step of gradient descent\r\n",
        "  optimizer.apply_gradients(zip(gradients, model.trainable_weights))\r\n",
        "\r\n",
        "  # Update the metric\r\n",
        "  train_metric.update_state(y_batch, y_batch_predict)\r\n",
        "\r\n",
        "  return batch_loss\r\n",
        "\r\n",
        "# Define the validation step as a separate function\r\n",
        "@tf.function\r\n",
        "def val_step(x_batch_val, y_batch_val):\r\n",
        "  y_batch_val_predict = model(x_batch_val, training=False)\r\n",
        "  validation_metric.update_state(y_batch_val, y_batch_val_predict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOdAp9l8B_FV",
        "outputId": "7668cf11-6462-45dd-adc1-30b1b95de0e1"
      },
      "source": [
        "# Exact same training loop as before, now just using a compiled version of the\r\n",
        "# time-critical components\r\n",
        "epochs = 10\r\n",
        "\r\n",
        "for epoch in range(epochs):\r\n",
        "  print(\"Working on epoch {} out of {} epochs:\\n\".format(epoch, epochs))\r\n",
        "  start_time = time.time()\r\n",
        "\r\n",
        "  # Now iterate over all batches\r\n",
        "  for batch_nr, (x_batch, y_batch) in enumerate(ds_train):\r\n",
        "\r\n",
        "    # Take one step of gradient descent\r\n",
        "    batch_loss = train_step(x_batch, y_batch)\r\n",
        "\r\n",
        "    # Record loss value every 100 batches\r\n",
        "    if batch_nr % 100 == 0:\r\n",
        "      print(\"Current training loss for batch {} is {: 2.4f}.\".format(batch_nr,\r\n",
        "                                                              batch_loss))\r\n",
        "          \r\n",
        "  # Accuracy at the end of the epoch\r\n",
        "  current_acc = train_metric.result()\r\n",
        "  print(\"\\nTraining accuracy: {: .4f}\".format(current_acc))\r\n",
        "\r\n",
        "  # Reset the metric at epoch's end\r\n",
        "  train_metric.reset_states()\r\n",
        "\r\n",
        "  # Validation accuracy at the end of the epoch\r\n",
        "  for x_batch_val, y_batch_val in ds_val:\r\n",
        "    val_step(x_batch_val, y_batch_val)\r\n",
        "\r\n",
        "  val_acc = validation_metric.result()\r\n",
        "  validation_metric.reset_states()\r\n",
        "  print(\"Validation accuracy: {: .4f}\".format(val_acc))\r\n",
        "  print(\"Time to complete: {: 2.4f}\\n\".format(time.time()-start_time))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Working on epoch 0 out of 10 epochs:\n",
            "\n",
            "Current training loss for batch 0 is  0.2973.\n",
            "Current training loss for batch 100 is  0.2498.\n",
            "Current training loss for batch 200 is  0.1861.\n",
            "Current training loss for batch 300 is  0.3251.\n",
            "Current training loss for batch 400 is  0.3829.\n",
            "\n",
            "Training accuracy:  0.9204\n",
            "Validation accuracy:  0.9255\n",
            "Time to complete:  0.9808\n",
            "\n",
            "Working on epoch 1 out of 10 epochs:\n",
            "\n",
            "Current training loss for batch 0 is  0.3378.\n",
            "Current training loss for batch 100 is  0.2113.\n",
            "Current training loss for batch 200 is  0.2382.\n",
            "Current training loss for batch 300 is  0.2174.\n",
            "Current training loss for batch 400 is  0.2888.\n",
            "\n",
            "Training accuracy:  0.9226\n",
            "Validation accuracy:  0.9272\n",
            "Time to complete:  0.6724\n",
            "\n",
            "Working on epoch 2 out of 10 epochs:\n",
            "\n",
            "Current training loss for batch 0 is  0.2366.\n",
            "Current training loss for batch 100 is  0.1104.\n",
            "Current training loss for batch 200 is  0.3008.\n",
            "Current training loss for batch 300 is  0.2348.\n",
            "Current training loss for batch 400 is  0.4214.\n",
            "\n",
            "Training accuracy:  0.9243\n",
            "Validation accuracy:  0.9292\n",
            "Time to complete:  0.6800\n",
            "\n",
            "Working on epoch 3 out of 10 epochs:\n",
            "\n",
            "Current training loss for batch 0 is  0.1838.\n",
            "Current training loss for batch 100 is  0.2655.\n",
            "Current training loss for batch 200 is  0.2753.\n",
            "Current training loss for batch 300 is  0.3037.\n",
            "Current training loss for batch 400 is  0.2242.\n",
            "\n",
            "Training accuracy:  0.9266\n",
            "Validation accuracy:  0.9299\n",
            "Time to complete:  0.6757\n",
            "\n",
            "Working on epoch 4 out of 10 epochs:\n",
            "\n",
            "Current training loss for batch 0 is  0.3019.\n",
            "Current training loss for batch 100 is  0.1763.\n",
            "Current training loss for batch 200 is  0.3204.\n",
            "Current training loss for batch 300 is  0.1826.\n",
            "Current training loss for batch 400 is  0.3202.\n",
            "\n",
            "Training accuracy:  0.9286\n",
            "Validation accuracy:  0.9325\n",
            "Time to complete:  0.7022\n",
            "\n",
            "Working on epoch 5 out of 10 epochs:\n",
            "\n",
            "Current training loss for batch 0 is  0.3021.\n",
            "Current training loss for batch 100 is  0.1701.\n",
            "Current training loss for batch 200 is  0.3589.\n",
            "Current training loss for batch 300 is  0.3430.\n",
            "Current training loss for batch 400 is  0.2260.\n",
            "\n",
            "Training accuracy:  0.9301\n",
            "Validation accuracy:  0.9336\n",
            "Time to complete:  0.7073\n",
            "\n",
            "Working on epoch 6 out of 10 epochs:\n",
            "\n",
            "Current training loss for batch 0 is  0.2510.\n",
            "Current training loss for batch 100 is  0.2522.\n",
            "Current training loss for batch 200 is  0.2326.\n",
            "Current training loss for batch 300 is  0.3437.\n",
            "Current training loss for batch 400 is  0.3438.\n",
            "\n",
            "Training accuracy:  0.9317\n",
            "Validation accuracy:  0.9354\n",
            "Time to complete:  0.6770\n",
            "\n",
            "Working on epoch 7 out of 10 epochs:\n",
            "\n",
            "Current training loss for batch 0 is  0.1974.\n",
            "Current training loss for batch 100 is  0.2759.\n",
            "Current training loss for batch 200 is  0.2243.\n",
            "Current training loss for batch 300 is  0.3528.\n",
            "Current training loss for batch 400 is  0.1844.\n",
            "\n",
            "Training accuracy:  0.9337\n",
            "Validation accuracy:  0.9357\n",
            "Time to complete:  0.6667\n",
            "\n",
            "Working on epoch 8 out of 10 epochs:\n",
            "\n",
            "Current training loss for batch 0 is  0.1791.\n",
            "Current training loss for batch 100 is  0.1984.\n",
            "Current training loss for batch 200 is  0.2151.\n",
            "Current training loss for batch 300 is  0.1383.\n",
            "Current training loss for batch 400 is  0.2100.\n",
            "\n",
            "Training accuracy:  0.9344\n",
            "Validation accuracy:  0.9364\n",
            "Time to complete:  0.6682\n",
            "\n",
            "Working on epoch 9 out of 10 epochs:\n",
            "\n",
            "Current training loss for batch 0 is  0.1764.\n",
            "Current training loss for batch 100 is  0.2011.\n",
            "Current training loss for batch 200 is  0.2121.\n",
            "Current training loss for batch 300 is  0.1764.\n",
            "Current training loss for batch 400 is  0.1809.\n",
            "\n",
            "Training accuracy:  0.9358\n",
            "Validation accuracy:  0.9374\n",
            "Time to complete:  0.6642\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}